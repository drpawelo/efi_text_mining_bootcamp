{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and Tokenising Text\n",
    "\n",
    "In this notebook:\n",
    "\n",
    "1. Loading a text file from your notebook or from a website\n",
    "2. Tokenising strings - splitting it into tokens (words etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap of syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list and get the first item and its length\n",
    "my_letters = [\"A\",\"B\",\"C\",\"D\",\"E\"] \n",
    "print(len(my_letters))\n",
    "print(my_letters[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice of items: from beginning until second from the end\n",
    "print(my_letters[:-2]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'comprehend' a list as its lowercase values \n",
    "my_letters_lowercased = [letter.lower() for letter in my_letters]\n",
    "print(my_letters_lowercased) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# also: run this cell now. It's the usual imports of text mining libraries\n",
    "\n",
    "import nltk\n",
    "import numpy\n",
    "import string\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Loading a text file from your notebook or from a website"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions & Objectives:\n",
    "\n",
    "- How can I load a text file from my harddrtive or a website?\n",
    "\n",
    "#### Key Points\n",
    "\n",
    "- To open and read a file on your computer, we use `open()`, `read()` and `close()`\n",
    "- To open and read a file from the internet, we use `urllib.request.urlopen()` and `.read().decode('utf-8')`\n",
    "- once the file is opened, you can store it's contents in a variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Broadly speaking there are two contexts in which we load a text file for analysis:\n",
    "\n",
    "- Local file:  you have your file on your 'virtual computer/haddrive' because you created or downloaded it earlier\n",
    "- Remove file: you access the file directly from some website, 'on the fly', processing it with your code, but never really saving it as your own. (eg. for copyright or convenience reasons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading a local file:\n",
    "\n",
    "First let's load some file from your 'hard drive' - because we are working inside of noteable, it acts as your harddrive. There's a file you downloaded called `file_inaugural_speech_obama.txt` and it is in the same folder as this notebook, so we're refering to it as `./file_inaugural_speech_obama.txt` (the `./` means 'same folder as this notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"./data/barack_obama_speeches/inaugural_speech.txt\"\n",
    "my_file = open(file_name) # open the file. \n",
    "speech = my_file.read() # read content of it and put them in a variable\n",
    "my_file.close() # close the file\n",
    "\n",
    "# after that you have access to that file as text in the speech variable you created\n",
    "print(\"number of characters:\", len(speech)) \n",
    "print(speech[:50]) # first 30 words\n",
    "print(speech[-50:]) # last 30 words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading a remote (online) file:\n",
    "To read the same file from an online source (like from the white house website) we need to import a url-handling library urllib, but the process is very simmilar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib # you have to only do this once\n",
    "link = \"https://raw.githubusercontent.com/drpawelo/efi_text_mining_bootcamp/master/data/inaugural_speech_obama.txt\"\n",
    "my_file = urllib.request.urlopen(link) # download the file (no need to open-close)\n",
    "speech = my_file.read().decode('utf-8') # read and decode content and save it\n",
    "\n",
    "# after that you have access to that file as text.\n",
    "print(len(speech)) # how long is it?\n",
    "print(speech[:50]) # first 30 words\n",
    "print(speech[-50:]) # last 30 words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's simmilar/different:\n",
    "\n",
    "Notice simmilarities and differences in both methods:\n",
    "\n",
    "**GET LIBRARIES/TOOLS** on top of what python already gives you. You do this only once per notebook.\n",
    "\n",
    "- \n",
    "\n",
    "**OPEN**:Both methods need a mame/address of the file (in folders, or on the website)\n",
    "\n",
    "- local:  `open(file_name)`\n",
    "- remote: `urllib.request.urlopen(link)`\n",
    "\n",
    "**READ**: In both methods once you have access to the file, you need to READ the content of it and put it in a string variable. Notice that remote files can come in various 'encodings' (ways to understand special characters and punctuation), so we usually specify the `UTF-8` (Unicode Transformation Format) for plain english. Another common one is `latin1`\n",
    "\n",
    "- local: `my_file.read()`\n",
    "- remote: `my_file.read().decode('utf-8')`\n",
    "\n",
    "**CLOSE**: only in the local file we need to close it once we've read it. It's so that another script or user can open it later. This works like with all the files on a computer: they can be opened just in one instance at a time.\n",
    "\n",
    "- local: `my_file.close()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üêõMinitask\n",
    "\n",
    "Write some python to open the following online file and display the characters between indeses 42380 and 42869 in that file. (don't peak what's in the file). Do you recognise what play this text is from?\n",
    "\n",
    "http://www.gutenberg.org/files/1513/1513-0.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# white your answer here:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So now we have a long string... what's next?\n",
    "\n",
    "But as we can see it is not particularly useful to operate on **Characters** as the main measure of length and to access parts of text. It would be more meaningful to ask for the first 10 words, or last 10 words. Indeed, we might want to include puctuation and symbols too.\n",
    "\n",
    "This is where tokens come in:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Tokenising strings - splitting it into tokens (words etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions & Objectives:\n",
    "\n",
    "- What is tokenisation?\n",
    "- How can a string of raw text be tokenised?\n",
    "\n",
    "#### Key Points\n",
    "\n",
    "- Tokenisation means to split a string into separate words and punctuation, for example to be able to count them.\n",
    "- Text can be tokenised using the a tokeniser, e.g. the punkt tokeniser in NLTK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to process text we need to break it down into tokens. As we explained at the start, a token is a letter, word, number, or punctuation which is contained in a string.\n",
    "\n",
    "To tokenise we first need to import the word_tokenize method from the tokenize package from NLTK which allows us to do this without writing the code ourselves.\n",
    "\n",
    "We will also download a specific tokeniser that NLTK uses as default. There are different ways of tokenising text and today we will use NLTK‚Äôs in-built punkt tokeniser by calling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell now. (it's fine if you see some pink warnings underneath it)\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's tokenise (split into tokens) a nursery rhyme \"Humpty Dumpty\".\n",
    "\n",
    "We will save the tokenised output in a list using the `humpty_tokens` variable and can inspect it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "humpty_string = \"Humpty Dumpty sat on a wall, Humpty Dumpty had a great fall; All the king's horses and all the king's men couldn't put Humpty together again.\"\n",
    "humpty_tokens = word_tokenize(humpty_string)\n",
    "print(humpty_tokens) # print all tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's print just a few of them to have a closer look:\n",
    "print(humpty_tokens[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unifying and cleaning up the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To further analyse the data, we'll first learn how to perforn some cleanup tasks. \n",
    "\n",
    "As you can see in the above example, some of the words are uppercase and some are lowercase. But Python is case-sensitive, which means that 'hope' and 'Hope' are considered to be two completely different strings.\n",
    "\n",
    "For example, when searching for a word or counting the occurrences of a word, we most likely will want to seek both for the lowercase and uppercase versions of the word (eg. `company` and `Company` ). That's why simplify the analysis, we often normalise the data and make it all lowercase. This way both of the above words would simply become `company` and will make the text easier to comprehend.\n",
    "\n",
    "Since our list of tokens is basically a list of strings (words) we can apply the `List comprehention Loop` we learned about before to transform our list of mixed-case words, into a list of lower-case words. \n",
    "\n",
    "As you might remember a syntax for such loop is `[output_format for item in items ]` where:\n",
    "\n",
    "- 'output_format' is some operation we perform on item, like `item.lower()` or `len(item)`\n",
    "- 'items' is the List with all the elements we want to transform\n",
    "- 'item' is a temporary name we give to each element of items, for the purposes of using that name inside of output_format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's modify above example, so that we only aquire lowe-case tokens of the nursery rhyme:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "humpty_string = \"Humpty Dumpty sat on a wall, Humpty Dumpty had a great fall; All the king's horses and all the king's men couldn't put Humpty together again.\"\n",
    "humpty_tokens = word_tokenize(humpty_string)\n",
    "lowercase_tokens = [token.lower() for token in humpty_tokens]\n",
    "print(lowercase_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üñáüí¨Buddy discussion: What would be the coolest text dataset to analyse?\n",
    "\n",
    "#### Ask your buddy now if they reached the **BUDDY TASK**. Once you both did, complete this task:\n",
    "\n",
    "Can each of you come up with ONE EXAMPLE of a text source that you would LOVE to have access to and analyse. Don't worry if it would be very hard (or impossible) to aquire, just imagine you have a magic wand. \n",
    "\n",
    "- eg. all the chats in Edinburgh taxis\n",
    "- eg. 1000 most popular recipies for apple pie\n",
    "- eg. transcripts of all job interviews for academic jobs in UK this year\n",
    "\n",
    "Don't spend too much time on this (max 2 mins) but take note of your favourite idea."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üêõMinitask\n",
    "\n",
    "Let's do our first, very simple piece of analysis. Do you think there were more mentions of \"We\" or \"They\" in the innaugural speech we looked at before?\n",
    "\n",
    "Let's try to re-use some pieces of code we wrote before and do our first very simple analysis:\n",
    "\n",
    "First without the lowercasing:\n",
    "\n",
    "- copy-paste your code from before to load the speech of president Obama.\n",
    "- use `word_tokenize()` on that variable, to turn it into a list of tokens.\n",
    "- count all occurances of a word 'we'. You can use the `a_list.count( a_word )` method like this:  `how_many_we = speech_tokens.count('we')`.\n",
    "- Print how many there were.\n",
    "- also count occurances of 'they'\n",
    "\n",
    "Which is the proportion of the usage of these words?\n",
    "\n",
    "- Now add the list-comprehention after you tokenised the text into a list, that will change list items into their lower-case equivalents. Do this after you tokenise the string, but before you do the counting.\n",
    "\n",
    "Now which word is more frequent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write yoru solution here \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ü¶ã Extra task (optional): if you have finished everything else already:\n",
    "\n",
    "What other words could you look for? Do you think you could create a list of words, like `['hope', 'fear' ,'can', 'cannot']` and use a for loop to print counts of all of these words in the speech?\n",
    "\n",
    "You can try to illustrate a particular point using data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
