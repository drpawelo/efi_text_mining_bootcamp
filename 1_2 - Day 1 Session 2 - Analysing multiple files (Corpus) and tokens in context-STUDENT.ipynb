{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this notebook:\n",
    "\n",
    "1. Working with a **Corpus** (multiple text files)\n",
    "2. **Concordance** Lists (tokens in context)\n",
    "3. Searching Text using **Regular Expressions** (wildcards syntax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell now. It's the usual imports of text mining libraries\n",
    "\n",
    "import nltk\n",
    "import numpy\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Working with a Corpus (multiple text files)\n",
    "\n",
    "#### Questions & Objectives:\n",
    "\n",
    "- How can I load a text collection made up of multiple text files and tokenise them?\n",
    "\n",
    "#### Key Points\n",
    "\n",
    "- To read an entire collection of text files you can use the PlaintextCorpusReader class/object provided by NLTK and its words() function to extract all the words from the text in the collection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text data comes in different forms. You might want to analyse a document in one file or an entire collection of documents (a corpus) stored in multiple files. You already know how to load a single document, and now you will see how to load an entire folder of text documents (a corpus)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Medical History of British India\n",
    "\n",
    "We will use the Medical History of British India collection provided by the National Libarry of Scotland as an example. It is located under a link below, if you'd like to read more about it, but for this course we have prepared it for you already. You will find it in the sub0folder inside your data folder.\n",
    "\n",
    "https://data.nls.uk/data/digitised-collections/a-medical-history-of-british-india/\n",
    "\n",
    "This dataset forms the first half of the Medical History of British India collection, which itself is part of the broader India Papers collection held by the Library. A Medical History of British India consists of official publications varying from short reports to multi-volume histories related to disease, public health and medical research between circa 1850 to 1950. These are historical sources for a period which witnessed the transition from a humoral to a biochemical tradition, which was based on laboratorial science and document the important breakthroughs in bacteriology, parasitology and the developments of vaccines in a colonial context.\n",
    "\n",
    "This collection has been made available as part of NLS‚Äôs DataFoundry platform which provides access to a number of their digitised collections. We are only interested in the text the Medical History of British India collection for this course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap: Loading a single file\n",
    "\n",
    "First, just to see one of the files, let's load one individual file and print a few of its tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('./data/Medical_History_of_British_India/74457530.txt') \n",
    "\n",
    "india_raw = file.read() \n",
    "india_tokens = word_tokenize(india_raw) \n",
    "lower_india_tokens = [word.lower() for word in india_tokens] \n",
    "print(lower_india_tokens[0:50] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading multiple files (a corpus) into a PlaintextCorpusReader object\n",
    "\n",
    "We can do the same for an entire collection of documents (a corpus). Instead of pointing to an individual file, we point to a directory/folder with many text documents in it. We will use the entire Medical History of British India collection as our dataset, which consists of almost 500 text documents stored in a folder.\n",
    "\n",
    "To read the text files in this collection we can use the `PlaintextCorpusReader` python class provided in the corpus package of NLTK that we imported before.\n",
    "\n",
    "You need to specify:\n",
    "\n",
    "- the collection directory name \n",
    "- a wildcard (generic name) for which files to read in the directory (e.g. `*` for all files, or `*.txt` for all text files `74*.txt` for all text files starting with `74`) \n",
    "- a text encoding of the files (in this case `latin1`) to indicate which alphabeth to use.\n",
    "\n",
    "when you use PlaintextCorpusReader it will look like this:\n",
    "\n",
    "`list_of_lists_of_tokens = PlaintextCorpusReader(folder_location, file_wildcard, encoding)`\n",
    "\n",
    "which in our example will look like this:\n",
    "\n",
    "`corpus_reader = PlaintextCorpusReader(\"./data/Medical_History_of_British_India\", '.*', encoding='latin1') `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import PlaintextCorpusReader\n",
    "corpus_root = \"./data/Medical_History_of_British_India\"\n",
    "corpus_reader = PlaintextCorpusReader(corpus_root, '.*', encoding='latin1') \n",
    "\n",
    "print(corpus_reader)\n",
    "# the output will make little sense, but do not worry, keep reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we tried to print our corpus, we saw something like `<PlaintextCorpusReader ... >` instead of a lot of words! Why? That's because the corpus we have loaded into variable `corpus_reader` is more than just a bunch of text variables: \n",
    "\n",
    "What you see when you try to print `corpus_reader` is the brief description of a `PlaintextCorpusReader object`\n",
    "\n",
    "**OBJECT is a type of a variable that combines some storage and some functionality**\n",
    "\n",
    "For example, the `PlaintextCorpusReader` object contains the list of lists of tokens (list of files, and each file is a list of words), and some additional information, and methods we can use. The one we will use the most is the `.words()` method to pull out all the tokens from the corpus.\n",
    "\n",
    "`all_tokens_in_corpus = my_corpus_reader.words()`\n",
    "\n",
    "or in our example:\n",
    "\n",
    "`corpus_tokens = corpus_reader.words() `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tokens = corpus_reader.words() \n",
    "print(corpus_tokens[400:450]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üêõMinitask\n",
    "\n",
    "\n",
    "- simulate opening the book in few places and reading a sentende: print a subsection (`your_list[ start : end ] `) of this corpus' tokens. Just make up an index to start reading it, like we did above (we made up the number `450` it has no signifficance).  \n",
    "\n",
    "eg. from the `corpus_tokens` list print 50 tokens starting at 1000th, or 9568th. Notice that it is not th emost efficient way to 'eyeball' the text corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your answer here:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing your tokens for analysis (eg. making all words lowercase)\n",
    "### A few words about overwhelming your computer with processing: DO NOT PANIC!\n",
    "\n",
    "Note that this dataset is Quite Large: it contains almost 500 files, 30.000.000 words, and 130.000.000 characters! It's 120MB of data.\n",
    "\n",
    "Still, loading all the files and turning text into tokens takes about 1-2 seconds!\n",
    "\n",
    "But when we lowercase all of the words in this corpus, we run a piece of code which turns every single character of the 130.000.000 characters into its lower case equivalent.\n",
    "\n",
    "`lower_corpus_tokens = [word.lower() for word in corpus_tokens]` \n",
    "\n",
    "But this is not going to happen instantly. Run the below code cell now, and then keep reading.\n",
    "\n",
    "You'll need to be patient, it might take even a minute or more to run. It will look as if your notebook has FROZEN (might stop responding), but it's just busy at work. You will know that the cell is running, because there will be an `In [*]` on the left top of that busy cell, and (on some browsers) the icon on the browser tab will turn into an hour glass.\n",
    "\n",
    "If you have not done it yet, run the below cell now and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_corpus_tokens = [word.lower() for word in corpus_tokens] \n",
    "print(lower_corpus_tokens[400:450]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some things you can do to not have to wait all the time for your code to finish:\n",
    "\n",
    "**We try to do the hardest processing only once, and store the result in a variable, which we use later** (instead of re-doing the processing all the time).\n",
    "\n",
    "And this is exactly what we do above: After the above line of code had finished running, the variable `lower_corpus_tokens` contains all of your tokens as lowercase characters.\n",
    "\n",
    "Accessing the tokens in this variable will take a very short time now, because all the time consuming processing (making things lowercase) is already done, and all the tokens are lowercase already - now we just want to see them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will execute almost immediately now, because all the processing \n",
    "# (changing millions of characters) is done, and we are just requesting a sub-set of a (very large) list:\n",
    "\n",
    "print(lower_corpus_tokens[400:450]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, when processing needs to be done, it needs to be done. So don't be scared of it, and when you see the `In [*]` indicator that a cell is processing, just get a cup of tea :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Concordance List for a text collection (contexts in which tokens appear)\n",
    "\n",
    "#### Questions & Objectives:\n",
    "\n",
    "- How can I load a text collection made up of multiple text files and tokenise them?\n",
    "\n",
    "#### Key Points\n",
    "\n",
    "- A concordance list is a list of all contexts in which a particular token appears in a corpus or text.\n",
    "- A concordance list can be created using the concordance() method of the Text` class in NLTK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words make a lot of sense in context. Concordance list is a fantastic way to glimpse into the text and see how a particular token is used.\n",
    "\n",
    "Next, we will display concordances for a particular token, i.e. all contexts a particular token appears in. We can do this using the Text class in NLTK‚Äôs text package. We can represent our list of lowercased tokens in the document collection loaded previously using the Text class. The concordance list of a token can be displayed using the `.concordance()` method on this class as shown below.\n",
    "\n",
    "Note that the process of aquiring concordance data will take abotu 10 seconds, depending on the how busy your current machine is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.text import Text\n",
    "\n",
    "text_of_the_corpus = Text(lower_corpus_tokens)\n",
    "print(text_of_the_corpus.concordance('woman'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the output for the next bit of code which creates a concordance list for the word ‚Äúhe‚Äù, we can see that there are many more results in the list than displayed on screen (Displaying 25 of 170 matches). The concordance() method only prints the first 25 results by default (or less if there are less)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_of_the_corpus = Text(lower_india_tokens)\n",
    "print(text_of_the_corpus.concordance('he'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can specify the number of lines using \n",
    "# an additional lines parameter, e.g.:\n",
    "print(text_of_the_corpus.concordance('he', lines=170))\n",
    "\n",
    "# notice that when the result of a cell is too long, it will become it's own little scrollable window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üêõMinitask: combining together what we learned so far\n",
    "\n",
    "This task will require you to copy-paste and adjust various lines of code from this notebook. We will load and analyse a collection of Barack Obama speeches. \n",
    "\n",
    "- load a new corpus: a few selected speeches of Barack Obama located in the folder `./data/barack_obama_speeches`. \n",
    "- turn corpus data into tokens (with `.words()`)\n",
    "- lowercase all the tokens using list comprehention loop (`[output for something in list_of somethings]`) and `.lower()`\n",
    "- create a Text object from the lowercased tokens with  `Text( your_lowercased_tokens )`\n",
    "- create concordance lists for some words that you find interesting, eg. 'hope', 'can', 'people'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# white your answer here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Searching text using Regular Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions & Objectives:\n",
    "\n",
    "- How can I search for tokens in text more flexibly? For example, to find all all mentions of `woman` and `women`, or all words starting with `multi`\n",
    "\n",
    "\n",
    "#### Key Points\n",
    "\n",
    "To search for tokens in text using regular expressions you need the `re` module and its search function.\n",
    "\n",
    "You will learn how to construct regular expressions. E.g. you can use a wildcard * or you can use a range of letters, e.g. [ae] (for a or e), [a-z] (for a to z), or numbers, e.g. [0-9] (for all single digits) etc. Regular expressions can be very powerful if used correctly. To find all mentions of the words woman or women you need to use the following regular expression wom[ae]n."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regular expressions are a very powerful tool, but we'll just give you a taster and some examples. For a more detailed overview and use of regular expressions, you can later refer to the Programming Historian lesson Understanding Regular Expressions https://programminghistorian.org/en/lessons/understanding-regular-expressions.\n",
    "\n",
    "**Regular expressions** are ways to be 'a bit vague' about text. (While being increadibly specific at the same time).\n",
    "\n",
    "For example Let's imagine we want to see all tokens that refer to `women` in text. If we were working with a person (not a computer) they might already assume I mean both singlular `woman` and plural `women`. But computers need us to be very very specific, and so we are provided with a way to describe small acceptable difference. This syntax is called regular expressions (RegEx).\n",
    "\n",
    "The way we arrive at regular expressions is a process of specifying what we want:\n",
    "\n",
    "- I could say: give me all occurances of `woman` and `women` and then add them all.\n",
    "- I could say: give me all occurances of `wom*n` where `*` is `a` or `e` \n",
    "- I could use regex to say give me all occurances of `wom[ae]n`\n",
    "- I could use regex to say give me all occurances of `^wom[ae]n$` which also means that there can be nothing before or after these characters, so `superwomen` and `womenhood` will not be included "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RegEx we will use is `^wom[ae]n$` and below we explain what it means:\n",
    "\n",
    "- `^` means: start here\n",
    "- `wom` and `n` look for these letters in this order\n",
    "- `[ae]` means: one character from this list, so `[ae]` means one character which is either `a` or `e`\n",
    "- `$` mean: end here\n",
    "\n",
    "This way we can look for the word `women` as well a `woman` in a corpus simultaneously, eg. to find out how many times they occur.\n",
    "\n",
    "Regular Expressions are usually used to define search terms in an 'a bit vague' but also 'very precisely specified' way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell now. It imports Regular Expressions module into this notebook\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we use RegEx on on a whole corpus let's first use it on some example data.\n",
    "\n",
    "Say I want to know if a given token matches/fits my RegEx. I can try to 'find' the match to that regex in my string.\n",
    "\n",
    "There are two possible outcomes of searching for a RegEx:\n",
    "\n",
    "- **Found it**: regex did find a match and returns a `re.Match` object (you can think of is as `True`)\n",
    "- **Not Found it**: regex did not find a match and returns `None`  (you can think of is as `False`)\n",
    "\n",
    "Basically, either a particular token fits your regex or it does not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(re.search('^wom[ae]n$', \"women\"))\n",
    "print(re.search('^wom[ae]n$', \"woman\"))\n",
    "print(re.search('^wom[ae]n$', \"something\")) # no match\n",
    "print(re.search('^wom[ae]n$', \"superwoman\")) # not exact match, so no match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regex is case-sensitive and that's why we lowercased our tokens first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(re.search('^wom[ae]n$', \"women\"))\n",
    "print(re.search('^wom[ae]n$', \"Women\"))\n",
    "print(re.search('^wom[ae]n$', \"WOMEN\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mini code recap: keeping only some elements from a list\n",
    "\n",
    "We'll use list comprehention's ability to filter list items using `if something_true_or_false`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print uppercase versions of every fruit in fruits\n",
    "fruits = [\"banana\", 'pinapple', 'plums', \"kiwi\"]\n",
    "new_fruits = [fruit.upper() for fruit in fruits]\n",
    "print(new_fruits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each fruit in fruits, return that fruit.upper(), \n",
    "# but only use items where fruit's first character is 'p'\n",
    "\n",
    "some_fruits = [fruit.upper() for fruit in fruits if fruit[0] == 'p']\n",
    "print(some_fruits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and to do the same thing, but without upper casing the words\n",
    "# for each fruit in fruits, return that fruit's name, \n",
    "# but only use items where fruit's first character is 'p'\n",
    "\n",
    "some_fruits = [fruit for fruit in fruits if fruit[0] == 'p']\n",
    "print(some_fruits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use `if fruit[0] == 'p'` because the comparison `fruit[0] == 'p'` returns `True` or `False`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using RegEx on a List of tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because RegEx also returns something like `True` or `False`, We will now use the same mechanism and the fact that re.search() returns something or nothing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, like in the above example we will:\n",
    "\n",
    "- filter the items in lower_india_tokens\n",
    "- keep only those which return `True` if we search for our RegEx in them (they match the RegEx)\n",
    "\n",
    "`[word \n",
    "for word in lower_india_tokens \n",
    "if re.search('^wom[ae]n$', word)]`\n",
    "\n",
    "Even thou it is a bit easier to read when split into 3 lines, traditionally we write it in one line:\n",
    "\n",
    "`[word for word in lower_india_tokens if re.search('^wom[ae]n$', word)]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "womaen_strings = [word for word in lower_india_tokens if re.search('^wom[ae]n$', word)]\n",
    "print(womaen_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if your code becomes too hard to read, you can add some new-lines to make it more readable. eg:\n",
    "womaen_strings = [word \n",
    "                  for word in lower_india_tokens\n",
    "                  if re.search('^wom[ae]n$', word)]\n",
    "print(womaen_strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the search results would change if you remove the `^` and `$` characters from the regular expression.\n",
    "\n",
    "Now that the results are stored in a list you can count them. We will see how to do that in the next section of the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "womaen_strings=[w for w in lower_india_tokens if re.search('wom[ae]n', w)]\n",
    "print(womaen_strings)\n",
    "# there should be at least one new item, can you see it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üñáüí¨Buddy discussion: What would be some useful ways you imagine RegEx could be used in your work/studies?\n",
    "\n",
    "#### Ask your buddy now if they reached the **BUDDY TASK**. Once you both did, complete this task:\n",
    "\n",
    "Can each of you come up with ONE OR TWO EXAMPLES of how the ability to use regular expressions could be useful to you?\n",
    "\n",
    "Don't spend too much time on this (max 2 mins) but take note of your favourite idea."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doing more with Regular Expressions: just a few examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regural expressions can be very specific and we will not cover them in detail here but they are very powerful to carry out complex searches, e.g. \n",
    "\n",
    "- find all tokens starting with a and are 12 characters long\n",
    "- find all tokens which are 13 characters long but that do not start with a lower case letter \n",
    "\n",
    "Some more RegEx syntax:\n",
    "\n",
    "- `.` means any character\n",
    "- `[abcd]` means a character which is either a, b, c or d\n",
    "- `[a-z]` means a letters between a-z\n",
    "- `[a-zA-Z]` means a letters between a-z and A-Z\n",
    "- `[0-9]` means a digit\n",
    "- `\\d` also means a digit\n",
    "\n",
    "\n",
    "- `*` means zero or more times\n",
    "- `+` means one or more times\n",
    "- `?` means zero or one time\n",
    "- `{5}` means 5 times\n",
    "- `{3,5}` means 3 to 5 times\n",
    "- `[^abc]` means anything but a,b or c\n",
    "\n",
    "Some examples:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A four letter word\n",
    "\n",
    "- `^[a-z]...$` means a 4 letter word\n",
    "- `^[a-z]{4}$` also means a 4 letter word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[word for word in lower_india_tokens if re.search('^[a-z]...$', word)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notice that we are returning the result, rather than printing it, because that puts them one under another\n",
    "# and makes them more readable. If we used print() it would look like this:\n",
    "\n",
    "print([word for word in lower_india_tokens if re.search('^[a-z]...$', word)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another example: any word starting with 'b', ending with 'y'. \n",
    "\n",
    "As in, between these letters `b` and `y` we expect any-character `.` to appears zero-or-more times `*` (which we write as `.*`)\n",
    "\n",
    "`'^b.*y$'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[word for word in lower_india_tokens if re.search('^b.*y$', word)]\n",
    "# replace * with a + to look for one or more letters between b and y, not zero or more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üêõMinitask: read RegEx with understanding\n",
    "\n",
    "You will wish you have this when solving crossword puzzles.\n",
    "\n",
    "In this task you will see some RegEx's and will try to explain what they do:\n",
    "\n",
    "example, explain RegEx `^[^a-g]..l.ing$`\n",
    "\n",
    "- find all 8 letter words that\n",
    "- do not start with a letters from a to c\n",
    "- and the fourth letter is 'n'\n",
    "- ends with 'ing'\n",
    "\n",
    "Run below code to see it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[word for word in lower_india_tokens if re.search('^[^a-c]..n.ing$', word)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run and explain below code\n",
    "\n",
    "[word for word in lower_india_tokens if re.search('^m[ae]n$', word)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[word for word in lower_india_tokens if re.search('^m[ae]n', word)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[word for word in lower_india_tokens if re.search('^d.*t$', word)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ü¶ã Extra task (optional): if you have finished everything else already:\n",
    "\n",
    "Either import a corpus that you would like to analyse youself (create a new folder inside of your `./data/` and put your files there), or use one of the two corpuses we looked at in this notebook \n",
    "\n",
    "Then investigate the context of some of the words and use RegEx to look for interesting patterns in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
