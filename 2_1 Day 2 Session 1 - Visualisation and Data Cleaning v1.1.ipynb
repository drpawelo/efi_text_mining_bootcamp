{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this notebook:\n",
    "\n",
    "1. Simple **Counting Tokens**\n",
    "2. Visualising **Frequency Distributions** (but first, **cleaning up the data**)\n",
    "3. Advanced visualisation: **Wordclouds**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Counting Tokens\n",
    "\n",
    "#### Questions & Objectives:\n",
    "\n",
    "- How can I count tokens in text?\n",
    "\n",
    "#### Key Points\n",
    "\n",
    "- To count tokens, one can make use of NLTK‚Äôs FreqDist class from the probability package. The N() method can then be used to count how many tokens a text or corpus contains.\n",
    "- Counts for a specific token can be obtained using fdist[\"token\"]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell now. It's the usual imports of text mining libraries\n",
    "\n",
    "import nltk\n",
    "import numpy\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's load the India corpus and lowercase it. this will take a minute to run\n",
    "\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "corpus_root = \"./data/Medical_History_of_British_India\"\n",
    "corpus_reader = PlaintextCorpusReader(corpus_root, '.*', encoding='latin1') \n",
    "corpus_tokens = corpus_reader.words() \n",
    "print(\"loaded tokens:\", len(corpus_tokens) )\n",
    "\n",
    "corpus_tokens = [word.lower() for word in corpus_tokens] \n",
    "print(\"finished lowercasing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting tokens in text\n",
    "\n",
    "You can also do other useful things like count the number of tokens in a text, determine the number and percentage count of particular tokens and plot the count distributions as a graph. To do this we have to import the FreqDist class from the NLTK probability package. When calling this class, a list of tokens from a text or corpus needs to be specified as a parameter in brackets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thiw will take a minute too\n",
    "from nltk.probability import FreqDist\n",
    "fdist = FreqDist(corpus_tokens)\n",
    "print(fdist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can print the counts of most common tokens with `freq_dist_object.most_common( how_many )`\n",
    "\n",
    "eg. `fdist.most_common(100)` for the most common 100 words\n",
    "\n",
    "The results will be arranged from top most frequent tokens, with and their frequency count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fdist.most_common(100))\n",
    "\n",
    "# fun fact: notice that this is a list of tupples [('word1',233), ('word2', 2324), ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instead of printing, you can just return the value from the cell,\n",
    "# it will be easier to read, but very long:\n",
    "fdist.most_common(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with `frequency_distribution.N()` we can count the total number of tokens in a corpus.\n",
    "\n",
    "eg. `fdist.N()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fdist.N())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `fdist[ your_word ]` you can count the number of times a token appears in a corpus:\n",
    "\n",
    "eg. `fdist['hospital']` returns `28280` which means that `word 'hospital' appears 28280 times in reports`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fdist['hospital'])\n",
    "print(fdist['he'])\n",
    "print(fdist['she'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `fdist.freq( your_word )` you can also determine the relative frequency of a token in a corpus, so what % of the corpus a term is:\n",
    "\n",
    "eg. `fdist.freq('hospital')` returns `0.000997673635341749` which means that `0.1% or all words are 'hospital'`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fdist.freq('hospital'))\n",
    "print(fdist.freq('he'))\n",
    "print(fdist.freq('she')) # notice this fraction is so small that it goes into 'scientific notation e-05'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note on counting tokens that match a Regular Expression\n",
    "\n",
    "If you have a list of tokens created using regular expression matching as in the previous section and you‚Äôd like to count them then you can also simply count the length of the list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will take a minute\n",
    "import re\n",
    "womaen_strings = [word for word in corpus_tokens if re.search('^wom[ae]n$', word)]\n",
    "print(len(womaen_strings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frequency counts of tokens are useful to compare different corpora in terms of occurrences of different words or expressions, for example in order to see if a word appears a lot rarer in one corpus versus another.\n",
    "\n",
    "Counts of tokens, documents and a entire corpus can also be used to compute simple pairwise document similarity of two documents (later, have a look at e.g. see Jana Vembunarayanan‚Äôs blogpost for a hands-on example of how to do that https://janav.wordpress.com/2013/10/27/tf-idf-and-cosine-similarity/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Visualising Frequency Distributions\n",
    "# (but first, cleaning up the data)\n",
    "\n",
    "\n",
    "#### Questions & Objectives:\n",
    "\n",
    "- How can I draw a frequency distribution of the most frequent words in a collection?\n",
    "- How can I visualise this data as a word cloud.\n",
    "\n",
    "#### Key Points\n",
    "\n",
    "- A frequency distribution can be created using the plot() method.\n",
    "- In this episode you have also learned how to clean data by removing stopwords and other types of tokens from the text.\n",
    "- A word cloud can be used to visualise tokens in text and their frequency in a different way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising Frequency distributions of tokens in text\n",
    "\n",
    "### Graph of the frequency of the words as they are:\n",
    "\n",
    "The plot() method can be called to draw the frequency distribution as a graph for the most common tokens in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist.plot(30,title='Frequency distribution for 30 most common tokens in our text collection')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the distribution contains a lot of non-content words like ‚Äúthe‚Äù, ‚Äúof‚Äù, ‚Äúand‚Äù etc. (we call these stop words) and punctuation. This is not very useful. Let's have a small peek on what these words are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist.most_common(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, definitely not useful. Many of these words look like noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üêõMinitask:  Identify 3-4 categories of not-very-helpful tokens in the above set (max 2 minutes)\n",
    "\n",
    "Look at the above set of most popular tokens. Many of them look important and meaningful 'government', 'disease' etc but many of them are not very useful.\n",
    "\n",
    "- identify some families of not helpful tokens and write names for these families below:\n",
    "\n",
    "Do not spend too much time on this (max 2 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here you can write yoru answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<details><summary style='color:blue'>CLICK HERE TO SEE THE THE ANSWER. BUT BUT REALLY TRY TO DO IT YOURSELF FIRST!</summary>\n",
    "\n",
    "    there's no code involved here. You might have come up with: numbers, punctuation, typical but not very meaningful words. Keep reading for more info  \n",
    "</details>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing tokens that are just noise:\n",
    "\n",
    "We can remove them before drawing the graph. We need to import stopwords from the corpus package to do this. The list of stop words is combined with a list of punctuation and a list of single digits using + signs into a new list of items to be ignored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some of python's build in 'cheat sheets' of punctuations and other 'meaningless charaters', and some provided by the nltk library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#let's have a look what are the words usually discarded:\n",
    "print(string.punctuation)\n",
    "print(string.digits)\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: we turn string to a list, so they can be added to the other list. But \n",
    "print(list(string.punctuation))\n",
    "print(list(string.digits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords, punctuation and digits. the set( ... ) syntax removes duplicates\n",
    "\n",
    "remove_these = set(stopwords.words('english') + list(string.punctuation) + list(string.digits))\n",
    "\n",
    "filtered_text = [word \n",
    "                 for word in corpus_tokens \n",
    "                 if not word in remove_these]\n",
    "\n",
    "# note: above broken-down 3-line version could be a one-liner (see below). \n",
    "# it's up to you, which format you prefer. above, or this:\n",
    "# filtered_text = [word for word in corpus_tokens if not word in remove_these]\n",
    "\n",
    "\n",
    "fdist_filtered = FreqDist(filtered_text)\n",
    "print(fdist_filtered.most_common(30))\n",
    "fdist_filtered.plot(30,title='Frequency distribution (excluding stopwords and punctuation)')\n",
    "\n",
    "# this graph should be a bit better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually adding more words to be ignored\n",
    "\n",
    "Above is already much better, but sometimes we want to manually add some words to be ignored. It is easy, we just need to add more elements to `remove_these` List.\n",
    "\n",
    "We looked at the top 100 words and found these to be not particularly useful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( [word for (word,count) in fdist_filtered.most_common(100)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to be some elements here that are not meaningful for our analysi:\n",
    "\n",
    "- rogue punctuation '...', '..', etc\n",
    "- a lot of two-digit numbers '12', 000\n",
    "- individual letters 'a', 'j' etc\n",
    "- some other not very meaningful words 'also','would'\n",
    "\n",
    "Let's create more lists of things we want to remove:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I create a range of numbers from 0 to 100 and turn them into strings, so they are like '45' not like 45\n",
    "numbers_1_to_100 = [str(integer) for integer in range(101)]\n",
    "print(numbers_1_to_100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here are the weird puctuations\n",
    "extra_punctuation_to_remove = ['.', '..','...','....','.....','......', ').', '.,']\n",
    "print(extra_punctuation_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "individual_letters = list(string.ascii_lowercase)\n",
    "print(individual_letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üêõMinitask: Identiy more words that are potentially noise\n",
    "\n",
    "In a minute we will remove tokens that are noise. Based on your previous minitask and the current most popular tokens:\n",
    "\n",
    "- identify 10 more words that are most likely noise\n",
    "- add them to the list `some_more_words_to_remove`, run the cell again and continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to see current 100 most popular tokens, as part of the task above \n",
    "# add some more words to some_more_words_to_remove\n",
    "\n",
    "print( [word for (word,count) in fdist_filtered.most_common(100)])\n",
    "\n",
    "some_more_words_to_remove = [ 'rs', 'per', 'would', '000']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<details><summary style='color:blue'>CLICK HERE TO SEE THE THE ANSWER. BUT BUT REALLY TRY TO DO IT YOURSELF FIRST!</summary>\n",
    "\n",
    "    ### BEGIN SOLUTION\n",
    "    some_more_words_to_remove = [ 'rs', 'per', 'would','one','two','first'\n",
    "                                 '000',  '00',  'co', 'ditto', '1st', 'ii', \n",
    "                                 'total', 'number', 'year', 'years']\n",
    "    ### END SOLUTION\n",
    "</details>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note about cleaning up data carefully\n",
    "Sometimes you might find words that do not communicate the content, like 'would' and we are showing you how to remove them here.\n",
    "\n",
    "But it comes with a warning: you have to be very careful persoming steps like these, because they have potential of completely biasing your data, but also careful cleaning of messy datasets is very important.\n",
    "\n",
    "Also: While it makes sense to remove stop words for this type of frequency analysis it essential to keep them in the data for other text analysis tasks. Retaining the original text is crucial, for example, when deriving part-of-speech tags of a text or for recognising names in a text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can re-do the visualisation again, this time using an expanded and customised list of items to ignore.\n",
    "This should be a more meaningful graph!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets combine it all together and generate our new graph\n",
    "\n",
    "remove_these = set(stopwords.words('english') + list(string.punctuation) + list(string.digits) \n",
    "        + numbers_1_to_100 + extra_punctuation_to_remove + individual_letters+some_more_words_to_remove)\n",
    "\n",
    "filtered_text = [word \n",
    "                 for word in corpus_tokens \n",
    "                 if not word in remove_these]\n",
    "    \n",
    "fdist_filtered = FreqDist(filtered_text)\n",
    "print(fdist_filtered.most_common(30))\n",
    "fdist_filtered.plot(30,title='Frequency distribution (excluding stopwords and punctuation)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Advanced visualisation: Wordclouds\n",
    "\n",
    "## Basic wordcloud:\n",
    "\n",
    "We can also present the filtered tokens as a word cloud. It's a modern type of graph where sizes of words communicate their frequency, often used fo have a quick an overview of the corpus. Additionally these wordcouds can be shaped or customised as we'll see below.\n",
    "\n",
    "We will use the `WordCloud( ).generate_from_frequencies()` method.  The input to this method is a frequency dictionary of all tokens and their counts in the text.\n",
    "\n",
    "You will also see another way to create a simplified frequency count of words. That's because wordcloud requires words to be in a dictionary format:\n",
    "\n",
    " `{'total': 94009, 'year': 82829, 'number': 63358, 'cases': 39876 .... }`\n",
    " \n",
    "We will use another python package `Counter` to create such dictionary using the `filtered_text` variable as input. Note it is much less powerful than FreqDist, but you might see it in other people's code, so we want you to be familiar with it.\n",
    "\n",
    "Once we have the data in the correct format, we generate the WordCloud using the frequency dictionary and plot the figure to size. We can show the plot using `plt.show()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "simple_frequencies_dict = Counter(filtered_text)\n",
    "\n",
    "# let's have a peek into this dictionary. How many times the word 'hospital' appears in filtered_text?\n",
    "print(simple_frequencies_dict['hospital'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### note on installing extra software on your virtual machine with !pip\n",
    "\n",
    "But first we'll show you one of the most powerful features of of jupyter notebooks: you can downlload and install almost any software from the internet into your 'virtual machine'. Because it is 'sandboxed' it is primarilly safe. To install things we use `!pip` python package installer command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this now, it will install wordcloud on your machine\n",
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "cloud = WordCloud(max_font_size=80,colormap=\"hsv\").generate_from_frequencies(simple_frequencies_dict)\n",
    "plt.figure(figsize=(16,12))\n",
    "plt.imshow(cloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# you should see a colourful diagram below. It is generate right on time for you. \n",
    "# go no, generate it again, you will see that it changes slightly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üñáüí¨Buddy discussion: What text data would wordclouds be really good and really bad at?\n",
    "\n",
    "#### Ask your buddy now if they reached the **BUDDY TASK**. Once you both did, complete this task:\n",
    "\n",
    "Try to identify strengths and weaknesses unique to the usage of wordclouds.\n",
    "\n",
    "Don't spend too much time on this (max 2 mins) but take note of your favourite idea."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fancy-shaped word cloud:\n",
    "\n",
    "And now a shaped word cloud for a bit of fun This will present your workcloud in the shape of a given image.\n",
    "\n",
    "You need a shape file which we provide for you in the form of the medical symbol (it looks like two snakes wrapped around a staff with wings)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mask image needs to have a transparent background so that only the black shape is used as a mask for the word cloud - parts that are black will be filled with your words, parts that are transparent will be left empty. You can use your own images later!\n",
    "\n",
    "It would be fun if we can customise:\n",
    "\n",
    "- the shape in which the words arrange themselves (ex, circle, shape of UK, question mark)\n",
    "- colours to use\n",
    "\n",
    "The **geeky bits** about how we use images and colours **(feel free to skip reading them)**:\n",
    "\n",
    "- To display the shaped word cloud you need to import the Image package form PIL as well as numpy. The image first needs to be opened and converted into a numpy array which we call med_mask. \n",
    "- A customised colour map (cmap) is created to present the words in black font. \n",
    "- Colours use #RRGGBB format where two hexadecimal characters (0123456789ABCDEF) describe amount of Red Green and Blue we want. eg. #FF0000 means full red, no green, no blue. #000000 means no neither, which is black. #FFFFFF is white, #111111 and #222222 are shades of grey, getting brighter as amount of colours increses\n",
    "- Then the word cloud is created with a white background, the mask and the colour map set as parameters and generated from the dictionary containing the number of occurrences for each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "# masking image\n",
    "medical_icon_mask_image = np.array(Image.open(\"./images/medical.png\"))\n",
    "\n",
    "# Custom Colormap\n",
    "colors = [\"#000000\", \"#111111\", \"#101010\", \"#121212\", \"#212121\", \"#222222\"]\n",
    "cmap = LinearSegmentedColormap.from_list(\"mycmap\", colors)\n",
    "\n",
    "# imcge details: background, shape-mask, colours \n",
    "wordcloud = WordCloud(background_color=\"white\", mask=medical_icon_mask_image, colormap=cmap)\n",
    "\n",
    "wordcloud.generate_from_frequencies(simple_frequencies_dict)\n",
    "plt.figure(figsize=(16,12))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ü¶ã Extra task (optional): if you have finished everything else already:\n",
    "\n",
    "- can you create a wordcloud of the presidential speeches corpus?\n",
    "- then, can you use your own masking image?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
