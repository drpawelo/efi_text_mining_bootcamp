{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this notebook:\n",
    "\n",
    "1. **Lexical Dispersion Plot** - where in the corpus a word appears\n",
    "2. Plotting **Frequency Over Time**\n",
    "3. **Collocations** of words - when they appear frequently near each other\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Lexical Dispersion Plot - where in the corpus a word appears\n",
    "\n",
    "\n",
    "#### Questions & Objectives:\n",
    "\n",
    "- How can I measure how frequently a word appears across the parts of a corpus?\n",
    "- How can I plot the occurrences of a word and how many words from the beginning of the corpus it appears?\n",
    "- We will use the US Presidential Inaugural Addresses and which are provided with NLTK.\n",
    "\n",
    "#### Key Points\n",
    "\n",
    "- Lexical dispersion is a visualisation that allows us to see where a particular term appears across a document or set of documents\n",
    "- We used NLTK‚Äôs dispersion_plot ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell now. It's the usual imports of text mining libraries\n",
    "\n",
    "import nltk\n",
    "import numpy\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot lexical dispersion of particular tokens.\n",
    "\n",
    "**Lexical dispersion is a measure of how frequently a word appears across the parts of a corpus**. \n",
    "\n",
    "This plot notes the occurrences of a word and how many words from the beginning of the corpus it appears (word offsets). This is particularly useful for a corpus that covers a longer time period and for which you want to analyse how specific terms were used more or less frequently over time.\n",
    "\n",
    "To create a lexical disperson plot, you will first load and import a different corpus, the inaugural corpus which are all US Presidential Inaugural Addresses and which are provided with NLTK: US Presidential Inaugural Addresses (1789-present)\n",
    "\n",
    "Many libraries you will use (for text mining, visualisation, etc) come with build-in data sets for you to practice. They are nice this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('inaugural')\n",
    "from nltk.corpus import inaugural\n",
    "from nltk.text import Text\n",
    "\n",
    "inaugural_tokens = inaugural.words()\n",
    "inaugural_texts = Text(inaugural_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create the lexical dispersion plot for this corpus you also need to load `dispersion_plot` from the `nltk.draw.dispersion` package.\n",
    "\n",
    "You can then call the dispersion_plot method given a set of parameters, including the target words you want to plot across the corpus, whether this should be done case-sensitively, and specifying the title of the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.draw.dispersion import dispersion_plot\n",
    "\n",
    "# the following command can be used to increase the size of the plot using width and hight specifications\n",
    "plt.figure(figsize=(12, 9))\n",
    "targets=['great','good','tax','work','change']\n",
    "\n",
    "dispersion_plot(inaugural_texts, targets, ignore_case=True, title='Lexical Dispersion Plot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üñáüêõBuddy task: What words might have been used only in some time periods?\n",
    "\n",
    "- adjust the above code to include other words. Remember these re innaugural speeches of USA presidents 1789-present. What words might have appeared over certain periods and not others. Try words like 'war', 'peace', 'freedom', 'women', 'slavery', 'god'\n",
    "\n",
    "Do not spend more than 2 minutes on this. Just try some words and move on. Things will get even more interesting in a minute.\n",
    "\n",
    "Notice that it is really annoying that we cannot see exactly the year when the particular word was heavilly used. We will solve thsat problem in the next section. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Plotting Frequency Over Time\n",
    "\n",
    "\n",
    "#### Questions & Objectives:\n",
    "\n",
    "- How can I extract and plot the frequency of specific terms over time?\n",
    "- how to use a NLTK‚Äôs ConditionalFreqDist class to extract the frequency of defined words.\n",
    "\n",
    "#### Key Points\n",
    "\n",
    "- We extract terms and the years from the files using NLTK‚Äôs ConditionalFreqDist class from the nltk.probability package\n",
    "- How to plot these on a graph to visualise how the use changes over time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nested loops: a new challenging python syntax\n",
    "\n",
    "This is a new Python syntaxt for loops inside of loops (nested loops), which is VERY CHALLENGING.\n",
    "\n",
    "So do not worry if you do not get it at first (don't spend more than 2 minutes on this) just move on to further tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell and then read through it. \n",
    "    \n",
    "# Goal: we have a set of fruit names, and a set of target letters,\n",
    "# each time a fruit contains a target letter, return them\n",
    "# eg. because 'pear' contains 'a' and 'p' return [('pear', 'p'), ('pear', 'a')]\n",
    "\n",
    "fruits = ['pear', \"banana\", \"kiwi\", 'apple' ]\n",
    "targets = ['a', 'p', 'w']\n",
    "\n",
    "new_words = [(fruit, target)\n",
    "            for fruit in fruits\n",
    "            for letter in fruit\n",
    "            for target in targets\n",
    "            if letter == target\n",
    "            ]\n",
    "print(new_words)\n",
    "\n",
    "# if this syntax is not clear, ask your buddy üñá, but even if it is not super clear,\n",
    "# you'll be fine, just continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to take meta-information from files to understand corpus better\n",
    "\n",
    "Similarly to lexical dispersion, you can also plot frequency of terms over time. This is similarly to the Google n-gram visualisation for the Google Books corpus but we will show you how to do something similar for your own corpus.\n",
    "\n",
    "You first need to import NLTK‚Äôs ConditionalFreqDist class from the nltk.probability package. To generate the graph, you have to specify the list of words to be plotted (see targets) and the x-axis labels (in this case the year the inaugural was held which appears at the start of each file: fileid[:4]).\n",
    "\n",
    "Note: file names ar ein format `1789-Washington.txt`, `1801-Jefferson.txt` so first 4 characters describe the year the speech was given\n",
    "\n",
    "The required data for the plot needs to be in format, where word is repeated for each year as many times as it was used that year, eg `freedom` was used 4 times in 1801 and twice in 1805:\n",
    "\n",
    "```\n",
    "[('freedom', '1801'),\n",
    " ('freedom', '1801'),\n",
    " ('freedom', '1801'),\n",
    " ('freedom', '1801'),\n",
    " ('freedom', '1805'),\n",
    " ('freedom', '1805'),\n",
    " ('freedom', '1809'),\n",
    "...\n",
    "```\n",
    "\n",
    "This dataset is created by:\n",
    "\n",
    "- return a tupple with a word and the year of the speech `(target, fileid[:4])`\n",
    "- for each **filename** (fileid) from the speeches set: `for fileid in inaugural.fileids()`\n",
    "- then for each **word** in that file `for word in inaugural.words(fileid)`\n",
    "- then for each **target** word in our specified target words\n",
    "- use that word **only if** word starts with the target `if word.lower().startswith(target))`\n",
    "    \n",
    "```\n",
    "[(target, fileid[:4])\n",
    "    for fileid in inaugural.fileids()\n",
    "    for word in inaugural.words(fileid)\n",
    "    for target in targets\n",
    "    if word.lower().startswith(target)]\n",
    "```\n",
    "    \n",
    "\n",
    "The ConditionalFreqDist object (cfd) stores the number of times each of the target words appear in the each of the speaches and the plot() method is used to visualise the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import ConditionalFreqDist\n",
    "\n",
    "# type this to set the figure size\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 9)\n",
    "\n",
    "targets=['great','good','tax','work','change']\n",
    "\n",
    "cfd = nltk.ConditionalFreqDist(\n",
    "    [(target, fileid[:4])\n",
    "    for fileid in inaugural.fileids()\n",
    "    for word in inaugural.words(fileid)\n",
    "    for target in targets\n",
    "    if word.lower().startswith(target)])\n",
    "\n",
    "cfd.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üêõMinitask: \n",
    "\n",
    "- change the words in the above graph. Use the words you discussed with your buddy above.\n",
    "\n",
    "- try to use regular expressions instead of specific words (see hints below)\n",
    "\n",
    "eg. if you wanted to compare together occurances of\n",
    "\n",
    "- words `man & men`\n",
    "- word `freedom`\n",
    "- any other words that start with `free` you could use targets:\n",
    "\n",
    "`targets=['^m[ea]n$', '^freedom$', '^free']`\n",
    "\n",
    "and instead of \n",
    "\n",
    "`if word.lower().startswith(target)])`\n",
    "\n",
    "use\n",
    "\n",
    "`if re.search(target, word.lower()))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copypaste the graph code to this cell and write your answer here\n",
    "import re\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Collocations\n",
    "\n",
    "\n",
    "#### Questions & Objectives:\n",
    "\n",
    "- How can I see what terms are often used together in a text or corpus?\n",
    "- We want to see words that collocate, occur together more often than by chance.\n",
    "- We will see what words co-occur within five words of each other.\n",
    "- We will then see which words appear more than ten times together.\n",
    "- We will then look at a measure to score the likelihood of these collocations being unusual.\n",
    "\n",
    "#### Key Points\n",
    "\n",
    "- We will use NLTK‚Äôs `BigramAssocMeasures()` and `BigramCollocationFinder` to find the words commonly found together in the US Presidential Inaugural Addresses set.\n",
    "- We will score these collocations using bigram_measures.likelihood_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may want to see what terms are often used together. We can do this by looking for collocations in a text, i.e. two word tokens occurring together in the text more often than would be expected by chance.\n",
    "\n",
    "For this we need to import the nltk.collocations module and more specifically BigramAssocMeasures() and BigramCollocationFinder. We allow a window of 5 words between collocated words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.collocations import BigramAssocMeasures\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "\n",
    "bigram_measures = BigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_words(inaugural_tokens, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then look for words that appear together 10 times or more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finder.apply_freq_filter(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A number of measures are available to score collocations or other associations including `bigram_measures.likelihood_ratio`. We apply this measure below and show the top ten collocated tokens (occuring in a window of 5 tokens with a frequency of 10 or more)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finder.nbest(bigram_measures.likelihood_ratio, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üêõMinitask:  Re-do the colocation analysis after removing stopwords, punctuation, etc\n",
    "\n",
    "Change the code below to display collocations in the inaugural speeches with these extras:\n",
    "\n",
    "- with all tokens in the inaugural_tokens being lowercased\n",
    "- after removing stopwords, punctuation and single digits\n",
    "\n",
    "Refer back to previous notebook for help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inaugural_tokens = inaugural.words()\n",
    "\n",
    "# HERE you will want to filter inaugural_tokens to not contain stopwords, punctuation, etc \n",
    "\n",
    "\n",
    "\n",
    "bigram_measures = BigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_words(inaugural_tokens, 5)\n",
    "finder.apply_freq_filter(10)\n",
    "finder.nbest(bigram_measures.likelihood_ratio, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
