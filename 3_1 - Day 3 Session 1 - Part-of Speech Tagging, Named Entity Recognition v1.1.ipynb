{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this notebook:\n",
    "\n",
    "1. **Part-of Speech Tagging Text**\n",
    "2. **Named Entity Recognition**\n",
    "3. extra: Python **Functions**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Part-of Speech Tagging Text\n",
    "\n",
    "#### Questions & Objectives:\n",
    "\n",
    "- How can I extract words that have a particular part of speech (POS) such as a noun or a verb?\n",
    "- How can I visualise those extracted words?\n",
    "- To understand what a part-of-speech (POS) is.\n",
    "- To use a POS tagger to label a corpus.\n",
    "- To extract words with a specific POS.\n",
    "- To visualise the extracted words using a using a frequence distribution graph and a word cloud.\n",
    "\n",
    "#### Key Points\n",
    "\n",
    "- We will use a NLTK‚Äôs part-of-speech tagger, ```averaged_perceptron_tagger```, to label each word with part-of-speech providing information on tense, number (plural/singular) and case.\n",
    "- We will use the text from the US Presidential Inaugural speeches, in particular that from the last speech (Trump's).\n",
    "- We will then extract all nouns both plural (NNS) and singular (NN).\n",
    "- We will  visualise the nouns extracted using a frequency distribution graph and a word cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell now. It's the usual imports of text mining libraries\n",
    "\n",
    "import nltk\n",
    "import numpy\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In text mining it can be useful to extract words that have a particular part of speech (POS) such as a noun or a verb. For example extracting all proper nouns can give use names and locations. This is done using a POS-tagger. \n",
    "\n",
    "**The POS-tag of a word is a label of the word indicating its part of speech as well as grammatical categories such as tense, number (plural/singular) and case. POS tagging is the process of automatically determining the POS-tags of the tokens in a corpus.**\n",
    "\n",
    "In this lesson, we will use NLTK‚Äôs `averaged_perceptron_tagger` as the POS-tagger. It uses the perceptron algorithm to predict which POS-tag is most likely given the word. We need to download the tagger in order to use it.\n",
    "\n",
    "The POS-tagger outputs tokens tagged with their POS-tag. It uses the [Penn Treebank POS tagset](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html) which is widely used for POS-tagging text.\n",
    "\n",
    "POS-tagging text is very useful when analysing a corpus or document and will allow us to do more in-depth analysis and visualisations. In order to `pos-tag` using NLTK, you also have to `import pos_tag` from the tag package.\n",
    "\n",
    "We are going to use the text from the US Presidential Inaugural speeches. This is a data set that we can download from NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's import the needed libraries\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.tag import pos_tag\n",
    "nltk.download('inaugural')\n",
    "from nltk.corpus import inaugural\n",
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This corpus comes in raw format but also is pre-tokenised. Therefore we can call the words() method to retrieve the tokenised text of all speeches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inaugural_tokens=inaugural.words()\n",
    "print(inaugural_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the tokens from the last inaugural, the one made by President Trump, by looking at the last member of the list of speeches using the fileids() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inaugural_tokens_trump = inaugural.words(inaugural.fileids()[0:-1])\n",
    "print(inaugural_tokens_trump)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can assign POS-tags to all speeches using NLTK‚Äôs pos_tag() method and view the first 20:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_inaugural_tokens = nltk.pos_tag(inaugural_tokens)\n",
    "tagged_inaugural_tokens[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what do these shortcuts mean? Official list is here https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "\n",
    "But here's a cheat sheet with English examples for all of us who do not have a degree in linguistics (yet): \n",
    "\n",
    "```\n",
    "CC coordinating conjunction\n",
    "CD cardinal digit\n",
    "DT determiner\n",
    "EX existential there (like: ‚Äúthere is‚Äù ‚Ä¶ think of it like ‚Äúthere exists‚Äù)\n",
    "FW foreign word\n",
    "IN preposition/subordinating conjunction\n",
    "JJ adjective ‚Äòbig‚Äô\n",
    "JJR adjective, comparative ‚Äòbigger‚Äô\n",
    "JJS adjective, superlative ‚Äòbiggest‚Äô\n",
    "LS list marker 1)\n",
    "MD modal could, will\n",
    "NN noun, singular ‚Äòdesk‚Äô\n",
    "NNS noun plural ‚Äòdesks‚Äô\n",
    "NNP proper noun, singular ‚ÄòHarrison‚Äô\n",
    "NNPS proper noun, plural ‚ÄòAmericans‚Äô\n",
    "PDT predeterminer ‚Äòall the kids‚Äô\n",
    "POS possessive ending parent‚Äôs\n",
    "PRP personal pronoun I, he, she\n",
    "PRP$ possessive pronoun my, his, hers\n",
    "RB adverb very, silently,\n",
    "RBR adverb, comparative better\n",
    "RBS adverb, superlative best\n",
    "RP particle give up\n",
    "TO, to go ‚Äòto‚Äô the store.\n",
    "UH interjection, errrrrrrrm\n",
    "VB verb, base form take\n",
    "VBD verb, past tense took\n",
    "VBG verb, gerund/present participle taking\n",
    "VBN verb, past participle taken\n",
    "VBP verb, sing. present, non-3d take\n",
    "VBZ verb, 3rd person sing. present takes\n",
    "WDT wh-determiner which\n",
    "WP wh-pronoun who, what\n",
    "WP$ possessive wh-pronoun whose\n",
    "WRB wh-abverb where, when\n",
    "```\n",
    "\n",
    "it's from  https://medium.com/@gianpaul.r/tokenization-and-parts-of-speech-pos-tagging-in-pythons-nltk-library-2d30f70af13b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then set up lists to hold specific parts of speech such as nouns. Firstly we set up an empty list and the we search for the nouns, NN singular and NNS plural. We can then print the first 20:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns = [] \n",
    "nouns = [word for (word, pos) in tagged_inaugural_tokens if (pos == 'NN' or pos == 'NNS')] \n",
    "nouns[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have created this list of nouns, we can plot their counts in the corpus as we did yesterday (see lesson on frequency counts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "fdist = FreqDist(nouns)\n",
    "fdist.plot(20,title='Frequency distribution for 20 most common nouns in the inaugural corpus')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üêõMinitask 1.1\n",
    "\n",
    "Plot the same but for the 30 most frequent nouns.  Take a look at the graph and spot any errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary style='color:blue'>CLICK HERE TO SEE THE ANSWER. BUT REALLY TRY TO DO IT YOURSELF FIRST!</summary>\n",
    "    \n",
    "    ### BEGIN SOLUTION\n",
    "    fdist.plot(30,title='Frequency distribution for 30 most common nouns in the inaugural corpus')\n",
    "    ### END SOLUTION\n",
    "    \n",
    "Notice one of the frequent nouns listed is apparently \"s\".  What could this be?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot the nouns as a word cloud like we did yesterday (see lesson on Counting tokens in text):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "cloud = WordCloud(max_font_size=60,colormap=\"hsv\").generate(' '.join(nouns))\n",
    "plt.rcParams[\"figure.figsize\"] = (16,12)\n",
    "plt.imshow(cloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üêõMinitask 1.2\n",
    "\n",
    "Change the code above to create a frequency list for the 10 most common adjectives in the inaugural corpus. The POS-tag for adjective ‚ÄòJJ‚Äô."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your solution here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary style='color:blue'>CLICK HERE TO SEE THE ANSWER. BUT REALLY TRY TO DO IT YOURSELF FIRST!</summary>\n",
    "    \n",
    "    ### BEGIN SOLUTION\n",
    "    adjectives = []\n",
    "    adjectives = [word for (word, pos) in tagged_inaugural_tokens if (pos == 'JJ')]\n",
    "    fdist = FreqDist(adjectives)\n",
    "    fdist.plot(10,title='Frequency distribution for 10 most common adjectives in the inaugural corpus')\n",
    "    ### END SOLUTION\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üêõMinitask 1.3\n",
    "\n",
    "Plot a word cloud of the adjectives in the inaugural corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your solution here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary style='color:blue'>CLICK HERE TO SEE THE ANSWER. BUT REALLY TRY TO DO IT YOURSELF FIRST!</summary>\n",
    "\n",
    "    ### BEGIN SOLUTION\n",
    "    cloud = WordCloud(max_font_size=60,colormap=\"hsv\").generate(' '.join(adjectives))\n",
    "    plt.rcParams[\"figure.figsize\"] = (16,12)\n",
    "    plt.imshow(cloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    ### END SOLUTION\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üêõMinitask 1.4\n",
    "\n",
    "You can do the same for another POS-tag. For the full list of Penn Treebank POS tags above in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your solution here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Named Entity Recognition\n",
    "\n",
    "\n",
    "#### Questions & Objectives:\n",
    "\n",
    "- How can I identify all the names in a corpus?\n",
    "- How can I visualise those names?\n",
    "- To understand what a named entity recognition is.\n",
    "- To understand how to use a named entity recogniser.\n",
    "- To extract named entities from text.\n",
    "- To visualise the extracted entities.\n",
    "\n",
    "#### Key Points\n",
    "\n",
    "- We will use spaCy's named entity recogniser to extract named entities from text.\n",
    "- We will continue to use the text from the US Presidential Inaugural speeches.\n",
    "- We will then extract certain entity types (e.g. all person names) and will see some errors in the output.\n",
    "- We will visualise the entities extraced using a word cloud.\n",
    "\n",
    "\n",
    "\n",
    "Named entity recognition (NER) or named entity tagging is a way of locating and labelling named entities in text (e.g. names of people, locations and organisations). NER can be used to identity networks of people mentioned in data or as the first step towards geo-parsing text, i.e. extracting and disambiguating locations mentioned in text.\n",
    "There are several off-the-shelf NER taggers that we can use. Here we will be using the [spaCy](https://spacy.io) tagger.\n",
    "\n",
    "To use it, you need to first install it and load the model required to run it. You also need to import spaCy, its models and load it. You should get a bunch of messages and if it worked it will say that the download and installation was successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U spacy\n",
    "!python3 -m spacy download en_core_web_sm\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have already loaded the inaugural corpus from NLTK as an example. So we will use its raw text (```inaugural.raw()```) as input to tag it with named entities (in this case names of people, organisations and locations). To do that we just call ```nlp()``` on that list of word tokens. This can take a few seconds as it has to process all of the inaugural corpus. The results are stored in the variable ```doc```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=inaugural.raw()\n",
    "doc=nlp(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then print the first 10 entities found in the text and their entity tags using a for loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(doc.ents[i].text + \", \" + doc.ents[i].label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can save the entities separately in a list of tuples by extracting them for the output ```doc``` and then print the first 20 entries of that list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ents=doc.ents\n",
    "named_entities = [(ent.text, ent.label_) for ent in ents] \n",
    "named_entities[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out all the different entity types that have been found here (ORG, DATE, ORDINAL, GPE = geopolitical entity etc. etc.). The complete named entity tagset can be found in the spaCy [documentation](https://spacy.io/api/annotation#named-entities)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üêõMinitask 2.1\n",
    "\n",
    "Print the last 20 entities in the inaugural corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your solution here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary style='color:blue'>CLICK HERE TO SEE THE ANSWER. BUT REALLY TRY TO DO IT YOURSELF FIRST!</summary>\n",
    "\n",
    "    ### BEGIN SOLUTION\n",
    "    # you need to specify the start counter to be 20 from the end (end minus 20)\n",
    "    # you can determine the end by using the len() method on the list\n",
    "    # the end counter can be left unspecified\n",
    "    # so the following statement returns the last 20 elements of the list\n",
    "    named_entities[len(named_entities)-20:]\n",
    "    ### END SOLUTION\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we will introduce the concept of function to make this easily repeatable for different types of entities.\n",
    "\n",
    "## 3. Functions: the most powerful tool in programmer's hands\n",
    "\n",
    "__Function:__ A function is a block of code that can be called to complete a task.\n",
    "\n",
    "We already used a lot of functions that python creators have created for us. like `print()` or `len()`. Notice what `len()` does:\n",
    "\n",
    "`length_of_list = len( [4,5,6] )`\n",
    "\n",
    "It's like a magic spell that Harry Potter has learned, and now he can use it in different contexts and on different things.\n",
    " \n",
    "For example, once Harry learned how to freeze things with the `glacius` spell, he can then use it on various objects: \n",
    "\n",
    "`ice = glacius( water )`\n",
    "\n",
    "`frozen_malfoy = glacius( malfoy )`\n",
    "\n",
    "etc. Notice that this spell **takes** some **ARGUMENTS** which are sort of like inputs, and **returns** something new or changed.\n",
    "\n",
    "The syntax of using a function is:\n",
    "\n",
    "`result = function_name( argument )` or even \n",
    "\n",
    "`result = function_name( argument1, argument2, argument3etc )`\n",
    "\n",
    "\n",
    "#### Creating our own functions:\n",
    "\n",
    "The most amazing thing is that we can create our own functions:\n",
    "\n",
    "In Python a function is defined using the ```def``` keyword. The name of the function is specified after ```def``` and the parameters passed to the function are specified in round brackets after the name.  The indented bit of code inside a function specifies everything that the function computes. A function can return data as a result of the computation it has performed using the ```return``` keyword.\n",
    "\n",
    "`def my_own_function( argument1, argument2, argument3):\n",
    "    do something with arguments\n",
    "    return some_value`\n",
    "\n",
    "The important detail is: when you define a function, it is like LEARNING a spell. You did not yet CALL/TRIGGER/CAST that spell. For example, it is possible to learn freezing spell without actually freezing things around you.\n",
    "\n",
    "- define a function: teach computer what to do `def something:`\n",
    "- call a function: use the skill you learned earlier `something()`\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here I define the function - notice nothing happened yet when you run this cell!\n",
    "def add_numbers( number1, number2):\n",
    "    print(\"Adding numbers now!\")\n",
    "    return number1+ number2\n",
    "\n",
    "# \"adding numbers now!\" is NOT printed yet, because function is defined, but not used (or called) yet!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here I call the function, it will return the value\n",
    "print(\"I'm about to do the adding\")\n",
    "my_result = add_numbers(3,5)\n",
    "print(\"I'm done adding\")\n",
    "print(my_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's another example of a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_first_number_larger( number1, number2):\n",
    "    if number1 > number2:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_result = is_first_number_larger(3,5)\n",
    "print(my_result)\n",
    "my_result = is_first_number_larger(12,5)\n",
    "print(my_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you could even put the return value of a function right into a print statement:\n",
    "\n",
    "print(is_first_number_larger(12,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üêõMinitask 3.1\n",
    "\n",
    "Write a function that tells you if a word is longer (in terms of its length of characters) than a number, like ```is_word_longer_than(word, number)``` that you could call like example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your function here, so that below code (which calls your function many time) works.\n",
    "\n",
    "\n",
    "\n",
    "# tests:\n",
    "print(is_word_longer_than(\"banana\", 4)) # should be true\n",
    "print(is_word_longer_than(\"banana\", 12)) # should be false\n",
    "print(is_word_longer_than(\"plum\", 3)) # should be true\n",
    "print(is_word_longer_than(\"plum\", 4)) # should be false\n",
    "print(is_word_longer_than(\"plum\", 5)) # should be false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary style='color:blue'>CLICK HERE TO SEE THE ANSWER. BUT REALLY TRY TO DO IT YOURSELF FIRST!</summary>\n",
    "\n",
    "    ### BEGIN SOLUTION\n",
    "    def is_word_longer_than(word, number):\n",
    "        if len(word) > number:\n",
    "            return True\n",
    "        else:\n",
    "           return False\n",
    "    ### END SOLUTION\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back into text mining using functions to help us with our work\n",
    "\n",
    "Below is the ```get_entities_of_type()``` function.  \n",
    "\n",
    "It contains code that extracts named entities of a specific type from the NER-tagged output.  This function takes as input the chosen ```type``` of entity and  ```named_entities``` list. It will use them to filter entities and return only the entities of the specified type marked up in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entities_of_type(type_we_seek, entities):\n",
    "    ents = [string for (string, tag) in entities if (tag == type_we_seek)] \n",
    "    # you can add indentation to the above line if you like\n",
    "    print(\"Number of strings tagged as \" + type_we_seek + \" \" + str(len(ents)))\n",
    "    return ents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the function, for example to select all entities tagged as ```PERSON```, you need to specify the type and the list of named entities to select from.  The output list can be stored in a new variable (```people```). You can then inspect the first few person entities found in the inaugural speeches and you will see that the tagger is not always correct (e.g. North, Providence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type=\"PERSON\"\n",
    "people = get_entities_of_type(type, named_entities)\n",
    "people[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the difference.\n",
    "\n",
    "Before we used functions:\n",
    "\n",
    "`type=\"PERSON\"\n",
    "persons = [string for (string, tag) in named_entities if (tag == type_we_seek)]\n",
    "type=\"ORG\"\n",
    "organisations = [string for (string, tag) in named_entities if (tag == type_we_seek)]`\n",
    "\n",
    "with a function:\n",
    "\n",
    "`type=\"PERSON\"\n",
    "persons = get_entities_of_type(type, named_entities)\n",
    "type=\"ORG\"\n",
    "organisations = get_entities_of_type(type, named_entities)`\n",
    "\n",
    "It's cleaner and prettier. But also right now our calculations took just one line. Imagine we needed 10 lines of calculations in the top bit ... these two examples would take us 20 lines of code\n",
    "\n",
    "Using a function you only need to define the code once and can execute it multiple times in different ways.\n",
    "\n",
    "## Rule of thumb: if you do something a lot, and catch yourself copy-pasting a lot of code, create a function for it, and extract all the 'variables' (things that vary) into function arguments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the entities extracted by their frequency or create word clouds for them (as we did in the previous lesson).  For example, we can create a word cloud just for the person names mentioned in the corpus.\n",
    "\n",
    "To do that we need to create a Counter dictionary containing the counts for each repetition of an entity.  A Counter is a subclass of a Dictionary and elements are stored as key and their counts as values.\n",
    "\n",
    "As you can see, spaCy some person names in the inaugural speeches more than others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "people_dict=Counter(people)\n",
    "print(\"Number of unique person names: \" + str(len(people_dict)))\n",
    "print(people_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud = WordCloud(max_font_size=50,colormap=\"hsv\").generate_from_frequencies(people_dict)\n",
    "plt.figure(figsize=(16,12))\n",
    "plt.imshow(cloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üêõMinitask 3.2\n",
    "\n",
    "Create a similar word cloud but for the geopolitical entities (GPE), i.e. countries, cities or states, in the corpus. You can reuse the function we created earlier to select GPE entities.  If you re-use code, remember to adjust the variable names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your solution here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary style='color:blue'>CLICK HERE TO SEE THE ANSWER. BUT REALLY TRY TO DO IT YOURSELF FIRST!</summary>\n",
    "\n",
    "    ### BEGIN SOLUTION\n",
    "    gpes = get_entities_of_type(\"GPE\", named_entities)\n",
    "    gpe_dict=Counter(gpes)\n",
    "    cloud = WordCloud(max_font_size=50,colormap=\"hsv\").generate_from_frequencies(gpe_dict)\n",
    "    plt.figure(figsize=(16,12))\n",
    "    plt.imshow(cloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    ### END SOLUTION\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ü¶ã Extra task (optional):\n",
    "\n",
    "look at the most common parts of code that we used in the notebooks and reused, and turn them into functions.\n",
    "\n",
    "These functions could be your tools that you will use later, like a personal crafted cheat-sheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your solution here\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
