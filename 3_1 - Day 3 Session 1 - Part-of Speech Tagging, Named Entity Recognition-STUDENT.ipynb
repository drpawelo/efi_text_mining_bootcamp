{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this notebook:\n",
    "\n",
    "1. **Part-of Speech Tagging Text**\n",
    "2. **Named Entity Recognition**\n",
    "3. extra: Python **Functions**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Part-of Speech Tagging Text\n",
    "\n",
    "#### Questions & Objectives:\n",
    "\n",
    "- How can I extract words that have a particular part of speech (POS) such as a noun or a verb?\n",
    "- How can I visualise those extracted words?\n",
    "- To understand what a part-of-speech (POS) is.\n",
    "- To use a POS tagger to lable a corpus.\n",
    "- To extract words with a specific POS.\n",
    "- To visualise the extracted words using a using a plot of frequence distribution and a word cloud.\n",
    "\n",
    "#### Key Points\n",
    "\n",
    "- We use a NLTK‚Äôs part-of-speech tagger, averaged_perceptron_tagger, to label each word with part of speech, tense, number, (plural/singular) and case.\n",
    "- We are used the text from the US Presidential Inaugaral speeches, in particular that from the last speech by Trump.\n",
    "- We then extracted all nouns both plural (NNS) and singular (NN).\n",
    "- We then visualise the nouns from these speeches using a plot of frequence distribution and a word cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell now. It's the usual imports of text mining libraries\n",
    "\n",
    "import nltk\n",
    "import numpy\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In text mining it can be useful to extract words that have a particular part of speech (POS) such as a noun or a verb. For example extracting all proper nouns can give use names and locations. This is done using a POS-tagger. \n",
    "\n",
    "**The POS-tag of a word is a label of the word indicating its part of speech as well as grammatical categories such as tense, number (plural/singular) and case. POS tagging is the process of automatically determining the POS-tags of the tokens in a corpus.**\n",
    "\n",
    "In this lesson, we will use NLTK‚Äôs `averaged_perceptron_tagger` as the POS-tagger. It uses the perceptron algorithm to predict which POS-tag is most likely given the word. We need to download the tagger in order to use it.\n",
    "\n",
    "The POS-tagger outputs tokens tagged with their POS-tag. It uses the `Penn Treebank POS tagset` which is widely used for POS-tagging text. (https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html)\n",
    "\n",
    "POS-tagging text is very useful when analysing a corpus or document and will allow us to do more indepth analysis and visualisations. In order to `pos-tag` using NLTK, you also have to `import pos_tag` from the tag package.\n",
    "\n",
    "We are going to use the text from the US Presidential Inaugaral speeches. This is a data set that we can download from NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's import the needed libraries\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.tag import pos_tag\n",
    "nltk.download('inaugural')\n",
    "from nltk.corpus import inaugural"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This corpus comes in raw format but also is pre-tokenised. Therefore we can call the words() method to retrieve the tokenised text of all speeches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inaugural_tokens=inaugural.words()\n",
    "print(inaugural_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the tokens from the last inaugural, the one made by President Trump, by looking at the last member of the list of speeches using the fileids() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inaugural_tokens_trump = inaugural.words(inaugural.fileids()[0:-1])\n",
    "print(inaugural_tokens_trump)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can assign POS-tags to all speeches using NLTK‚Äôs pos_tag() method and view the first 20:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_inaugural_tokens = nltk.pos_tag(inaugural_tokens)\n",
    "tagged_inaugural_tokens[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what do these shortcuts mean? Official list is here https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "\n",
    "But here's a cheatsheet with English examples for all of us who do not have a degree in linguistics (yet): \n",
    "\n",
    "```\n",
    "CC coordinating conjunction\n",
    "CD cardinal digit\n",
    "DT determiner\n",
    "EX existential there (like: ‚Äúthere is‚Äù ‚Ä¶ think of it like ‚Äúthere exists‚Äù)\n",
    "FW foreign word\n",
    "IN preposition/subordinating conjunction\n",
    "JJ adjective ‚Äòbig‚Äô\n",
    "JJR adjective, comparative ‚Äòbigger‚Äô\n",
    "JJS adjective, superlative ‚Äòbiggest‚Äô\n",
    "LS list marker 1)\n",
    "MD modal could, will\n",
    "NN noun, singular ‚Äòdesk‚Äô\n",
    "NNS noun plural ‚Äòdesks‚Äô\n",
    "NNP proper noun, singular ‚ÄòHarrison‚Äô\n",
    "NNPS proper noun, plural ‚ÄòAmericans‚Äô\n",
    "PDT predeterminer ‚Äòall the kids‚Äô\n",
    "POS possessive ending parent‚Äôs\n",
    "PRP personal pronoun I, he, she\n",
    "PRP$ possessive pronoun my, his, hers\n",
    "RB adverb very, silently,\n",
    "RBR adverb, comparative better\n",
    "RBS adverb, superlative best\n",
    "RP particle give up\n",
    "TO, to go ‚Äòto‚Äô the store.\n",
    "UH interjection, errrrrrrrm\n",
    "VB verb, base form take\n",
    "VBD verb, past tense took\n",
    "VBG verb, gerund/present participle taking\n",
    "VBN verb, past participle taken\n",
    "VBP verb, sing. present, non-3d take\n",
    "VBZ verb, 3rd person sing. present takes\n",
    "WDT wh-determiner which\n",
    "WP wh-pronoun who, what\n",
    "WP$ possessive wh-pronoun whose\n",
    "WRB wh-abverb where, when\n",
    "```\n",
    "\n",
    "it's from  https://medium.com/@gianpaul.r/tokenization-and-parts-of-speech-pos-tagging-in-pythons-nltk-library-2d30f70af13b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then set up lists to hold specific parts of speech such as nouns. Firstly we set up an empty list and the we search for the nouns, NN singular and NNS plural. We can then print the first 20:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns = [] \n",
    "nouns = [word for (word, pos) in tagged_inaugural_tokens if (pos == 'NN' or pos == 'NNS')] \n",
    "nouns[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have created this list of nouns, we can plot their counts in the corpus as we did yesterday (see lesson on frequency counts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "fdist = FreqDist(nouns)\n",
    "fdist.plot(30,title='Frequency distribution for 30 most common nouns in the inaugural corpus')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot the nouns as a word cloud like we did yesterday (see lesson on Counting tokens in text):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "cloud = WordCloud(max_font_size=60,colormap=\"hsv\").generate(' '.join(nouns))\n",
    "plt.rcParams[\"figure.figsize\"] = (16,12)\n",
    "plt.imshow(cloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üêõMinitask\n",
    "\n",
    "Change the code above to create a frequency list for the most common adjectives in the inaugural corpus. The POS-tag for adjective ‚ÄòJJ‚Äô."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üêõMinitask\n",
    "\n",
    "Plot a word cloud of the adjectives in the inaugural corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üêõMinitask\n",
    "\n",
    "You can do the same for another POS-tag. For the full list of Penn Treebank POS tags above in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Named Entity Recognition\n",
    "\n",
    "Named entity recognition (NER) or named entity tagging is a way of locating and classifying named entities in text (e.g. names of people, locations and organisations). NER can be used to identity networks of people mentioned in data or as the first step towards geo-parsing text, i.e. extracting and disambiguating locations mentioned in text.\n",
    "There are several off-the-shelf NER taggers that we can use. Here we will be using the spaCy tagger.\n",
    "\n",
    "To use it, you need to first install it and load the model required to run it. You should have installed spacy as part of the [Prerequisites](https://github.com/bea-alex/text-mining-course/blob/master/prerequisites.md) but if not then go back to do that now before you import spacy and load the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -U spacy\n",
    "!python3 -m spacy download en_core_web_sm\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have already loaded the inaugural corpus from NLTK as an example. So we will use their raw text (inaugural.raw()) as input to tag it with named entities (in this case names of people, organisations and locations). To do that we just call nlp() on that list of word tokens. This can take a few seconds as it has to process all of the inaugural corpus. The results are stored in the variable doc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=inaugural.raw()\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then print the first 10 entities found in the text and their entity tags using a for loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(doc.ents[i].text + \", \" + doc.ents[i].label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can save the entities separately in a list of tuples by extracting them for the output ```doc``` and then print the first 50 entries of that list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ents=doc.ents\n",
    "named_entities = [(ent.text, ent.label_) for ent in ents] \n",
    "named_entities[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can do the same for specific entity types (e.g. organisation (ORG) or person (PER).  At this point we will introduce the concept of function to make this easily repeatable for different types.\n",
    "\n",
    "## Functions: the most powerful tool in programmer's hands:\n",
    "\n",
    "__Function:__ A function is a block of code that can be called to complete a task.\n",
    "\n",
    "We already used a lot of functions that python creators have created for us. like `print()` or `len()`. Notice what `len()` does:\n",
    "\n",
    "`length_of_list = len( [4,5,6] )`\n",
    "\n",
    "It's like a magic spell that Harry Potter has learned, and now he can use it in different context and on different things.\n",
    " \n",
    "eg. once Harry learned how to freeze things with `glacius` spell, he can then use it on various objects: \n",
    "\n",
    "`ice = glacius( water )`\n",
    "\n",
    "`frozen_malfoy = glacius( malfoy )`\n",
    "\n",
    "etc. Notice that this spell **takes** some **ARGUMENTS** which are sort of like inputs, and **returns** something new or changed.\n",
    "\n",
    "Overall syntaxt of using a function is \n",
    "\n",
    "`result = function_name( argument )` or even \n",
    "\n",
    "`result = function_name( argument1, argument2, argument3etc )`\n",
    "\n",
    "\n",
    "#### Creating our own functions:\n",
    "\n",
    "The most amazing thing is that we can create our own functions:\n",
    "\n",
    "In Python a function is defined using the ```def``` keyword. The name of the function is specified after ```def``` and the parameters passed to the function are specified in round brackets after the name.  The indented bit of code after the function specifies everything that the function computes. A function can return data as a result of the computation it has performed using the ```return``` keyword.\n",
    "\n",
    "`def my_own_function( argument1, argument2, argument3):\n",
    "    do something with arguments\n",
    "    return some_value`\n",
    "\n",
    "The important detail is: when you define a function, it is like LEARNING a spell. You did not yet CALL/TRIGGER/CAST that spell. eg. it is possible to learn freezing spell without actually freezing things around you.\n",
    "\n",
    "- define a function: teach computer what to do `def something:`\n",
    "- call a function: use the skill you learned earlier `something()`\n",
    "\n",
    "for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here I define the function - notice nothing happened yet when you run this cell!\n",
    "def add_numbers( number1, number2):\n",
    "    print(\"adding numbers now!\")\n",
    "    return number1+ number2\n",
    "\n",
    "# \"adding numbers now!\" is NOT printed yet, because function is defined, but not used yet!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here I call the function, it will return the value\n",
    "print(\"I'm about to do the adding\")\n",
    "my_result = add_numbers(3,5)\n",
    "print(\"I'm done adding\")\n",
    "print(my_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's anbother example of a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_first_number_larger( number1, number2):\n",
    "    if number1 > number2:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_result = is_first_number_larger(3,5)\n",
    "print(my_result)\n",
    "my_result = is_first_number_larger(12,5)\n",
    "print(my_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you could even put the return value of a function right into a print:\n",
    "\n",
    "print(is_first_number_larger(12,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üêõMinitask\n",
    "\n",
    "Write a function that tells you if a word is longer than a number, like is_word_longer_than(word, number) that you could call like example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here write your function, so that below code works. Then call your function many times\n",
    "\n",
    "\n",
    "# tests:\n",
    "print(is_word_longer_than(\"banana\", 4)) # should be true\n",
    "print(is_word_longer_than(\"banana\", 12)) # should be false\n",
    "print(is_word_longer_than(\"plum\", 3)) # should be true\n",
    "print(is_word_longer_than(\"plum\", 4)) # should be false\n",
    "print(is_word_longer_than(\"plum\", 5)) # should be false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back into text mining: Using functions to help us with our work:\n",
    "\n",
    "Below is the ```get_entities_of_type()``` function.  \n",
    "\n",
    "It contains code that extracts named entities of a specific type from the NER-tagged output.  This function takes as input the ```type``` and  ```named_entities``` list. It will use them to filter entities and return only the entities of the specified type marked up in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entities_of_type(type_we_seek, entities):\n",
    "    ents = [string for (string, tag) in entities if (tag == type_we_seek)] \n",
    "    # you can add indentation to the above line if you like\n",
    "    print(\"Number of strings tagged as \" + type_we_seek + \" \" + str(len(ents)))\n",
    "    return ents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the function you need to specify the type and the output list can be stored in a new variable (people). You can then inspect the first few person entities and you will see that the tagger is not always correct (e.g. Jealous, Distinct)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type=\"PERSON\"\n",
    "people = get_entities_of_type(type, named_entities)\n",
    "people[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the difference.\n",
    "\n",
    "Before we used functions:\n",
    "\n",
    "`type=\"PERSON\"\n",
    "persons = [string for (string, tag) in named_entities if (tag == type_we_seek)]\n",
    "type=\"ORG\"\n",
    "organisations = [string for (string, tag) in named_entities if (tag == type_we_seek)]`\n",
    "\n",
    "with a function:\n",
    "\n",
    "`type=\"PERSON\"\n",
    "persons = get_entities_of_type(type, named_entities)\n",
    "type=\"ORG\"\n",
    "organisations = get_entities_of_type(type, named_entities)`\n",
    "\n",
    "It's just cleaner and prettier. But also right now our calculations took just one line. Imagine we needed 10 lines of calculations in the top bit... these two examples would take us 20 lines of code.\n",
    "\n",
    "But with functions it would take just 2 lines (2 calls of the function), and some extra lines to define it.\n",
    "\n",
    "## Rule of thumb: if you do something a lot, and catch yourself copy-parting a lot of code, create a function for it, and extract all the 'variables' (things that vary) into function arguments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the entites extracted by their frequency or create word clouds for them (as we did in the previous lesson).  For example, we can create a word cloud just for the person names mentioned in the corpus.\n",
    "\n",
    "To do that we need to create a Counter dictionary containing the counts for each repetition of an entity.  A Counter is a subclass of a Dictionary and elements are stored as key and their counts as values.\n",
    "\n",
    "As you can see, spaCy recognised 109 unique person names in the inaugural speeches, some of them occurring more than others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "people_dict=Counter(people)\n",
    "print(\"Number of unique person names: \" + str(len(people_dict)))\n",
    "print(people_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud = WordCloud(max_font_size=50,colormap=\"hsv\").generate_from_frequencies(people_dict)\n",
    "plt.figure(figsize=(16,12))\n",
    "plt.imshow(cloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üêõMinitask: \n",
    "\n",
    "Create a similar word cloud but for the location names in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ü¶ã Extra task (optional):\n",
    "\n",
    "look at the most common parts of code that we used in the notebooks and reused, and turn them into functions.\n",
    "\n",
    "These functions could be your tools that you will use later, like a personal crafted cheat-sheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can code here:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
